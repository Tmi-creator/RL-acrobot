{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T20:21:34.209032Z",
     "start_time": "2026-02-19T20:21:34.199369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    COLAB = False\n",
    "    pass\n",
    "\n",
    "if COLAB:\n",
    "    !pip -q install swig\n",
    "    !pip -q install \"gymnasium[classic-control, atari, box2d]\"\n",
    "    !pip -q install piglet\n",
    "    !pip -q install imageio_ffmpeg\n",
    "    !pip -q install moviepy==1.0.3"
   ],
   "id": "80ffa8af908e1d14",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "aa4a81a2-b188-47be-8b99-66a95401eed4",
   "metadata": {
    "id": "aa4a81a2-b188-47be-8b99-66a95401eed4",
    "ExecuteTime": {
     "end_time": "2026-02-19T20:21:42.018860Z",
     "start_time": "2026-02-19T20:21:34.247820Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "if COLAB:\n",
    "    from google.colab import files\n",
    "    from google.colab.patches import cv2_imshow\n",
    "    from google.colab import output\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    import os\n",
    "\n",
    "    SAVE_DIR = \"/content/drive/MyDrive/acrobot_grid\"\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "    print(\"Saving to:\", SAVE_DIR)\n",
    "else:\n",
    "    SAVE_DIR = \"/acrobot_grid\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "51cbe381-b616-4359-8ebc-ab8ca63302ef",
   "metadata": {
    "id": "51cbe381-b616-4359-8ebc-ab8ca63302ef",
    "ExecuteTime": {
     "end_time": "2026-02-19T20:21:42.044804Z",
     "start_time": "2026-02-19T20:21:42.032498Z"
    }
   },
   "source": [
    "def norm_state(s):\n",
    "    s = np.array(s, dtype=np.float32).copy()\n",
    "    if s.shape[0] >= 6:\n",
    "        s[4] /= (4 * np.pi)\n",
    "        s[5] /= (9 * np.pi)\n",
    "    return s\n",
    "\n",
    "\n",
    "def select_action_eps_greedy(Q, state, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return int(np.random.randint(0, Q.n_actions))\n",
    "\n",
    "    if not isinstance(state, torch.Tensor):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        return int(Q(state).argmax().item())\n",
    "\n",
    "\n",
    "def to_tensor(x, dtype=np.float32):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x\n",
    "    x = np.asarray(x, dtype=dtype)\n",
    "    x = torch.from_numpy(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def compute_td_loss(Q, states, actions, td_target, weights=None):\n",
    "    s = to_tensor(states)  # [B, S]\n",
    "    a = to_tensor(actions, int).long()  # [B]\n",
    "\n",
    "    Q_s = Q(s)\n",
    "    Q_s_a = Q_s.gather(1, a.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    td_error = Q_s_a - td_target\n",
    "\n",
    "    td_losses = F.smooth_l1_loss(Q_s_a, td_target, reduction='none')\n",
    "    w = torch.tensor(weights, dtype=torch.float32, device=td_losses.device)\n",
    "    loss = (td_losses * w).mean()\n",
    "    return loss, torch.abs(td_error).detach()\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "c66ee2bd-8528-4c72-8c27-20f7aad5286d",
   "metadata": {
    "id": "c66ee2bd-8528-4c72-8c27-20f7aad5286d",
    "ExecuteTime": {
     "end_time": "2026-02-19T20:21:42.063775Z",
     "start_time": "2026-02-19T20:21:42.056234Z"
    }
   },
   "source": [
    "def eval_dqn(env_name, Q, n_episodes=10, seed=0):\n",
    "    env = gym.make(env_name)\n",
    "    rets = []\n",
    "    for ep in range(n_episodes):\n",
    "        s, _ = env.reset(seed=seed + ep)\n",
    "        done, ep_return = False, 0.\n",
    "\n",
    "        while not done:\n",
    "            # set epsilon = 0 to make an agent act greedy\n",
    "            s = norm_state(s)\n",
    "            a = select_action_eps_greedy(Q, s, epsilon=0.)\n",
    "            s_next, r, terminated, truncated, _ = env.step(a)\n",
    "            done = terminated or truncated\n",
    "            ep_return += r\n",
    "            s = s_next\n",
    "        rets.append(ep_return)\n",
    "    env.close()\n",
    "    return float(np.mean(rets))"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "3ae2d18b-bb97-4a8f-94ee-259935c039b9",
   "metadata": {
    "id": "3ae2d18b-bb97-4a8f-94ee-259935c039b9",
    "ExecuteTime": {
     "end_time": "2026-02-19T20:21:42.092964Z",
     "start_time": "2026-02-19T20:21:42.079519Z"
    }
   },
   "source": [
    "def sample_prioritized_batch(replay_buffer, n_samples, candidate_size=4096, alpha=0.6, beta=0.4):\n",
    "    n = len(replay_buffer)\n",
    "    k = min(candidate_size, n)\n",
    "    cand = np.random.randint(0, n, size=k)\n",
    "    priorities = np.array([replay_buffer[i][0] for i in cand], dtype=np.float32)\n",
    "    priorities = (np.abs(priorities) + 1e-6) ** alpha\n",
    "    s = priorities.sum()\n",
    "    if not np.isfinite(s) or s <= 0:\n",
    "        priorities = np.ones_like(priorities) / len(priorities)\n",
    "    else:\n",
    "        priorities /= s\n",
    "    idx_in_cand = np.random.choice(k, size=n_samples, replace=n_samples > k, p=priorities)\n",
    "    sample_probs = priorities[idx_in_cand]\n",
    "\n",
    "    weights = (1.0 / (sample_probs + 1e-12)) ** beta\n",
    "    weights = weights / weights.max()\n",
    "    weights = weights.astype(np.float32)\n",
    "\n",
    "    indices = cand[idx_in_cand]\n",
    "    v = [replay_buffer[i] for i in indices]\n",
    "    _, states, actions, rewards, next_states, terminateds, n_steps = zip(*v)\n",
    "\n",
    "    batch = (\n",
    "        np.array(states, dtype=np.float32),\n",
    "        np.array(actions, dtype=np.int64),\n",
    "        np.array(rewards, dtype=np.float32),\n",
    "        np.array(next_states, dtype=np.float32),\n",
    "        np.array(terminateds, dtype=np.bool),\n",
    "        np.array(n_steps, dtype=np.int64),\n",
    "    )\n",
    "    return batch, indices, weights\n",
    "\n",
    "\n",
    "def update_batch(replay_buffer, indices, new_priority):\n",
    "    \"\"\"Updates batches with corresponding indices\n",
    "    replacing their priority values.\"\"\"\n",
    "    for i, idx in enumerate(indices):\n",
    "        replay_buffer[idx] = (new_priority[i],) + replay_buffer[idx][1:]"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T20:21:42.127335Z",
     "start_time": "2026-02-19T20:21:42.106406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std_init=0.5):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.empty(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.empty(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.empty(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n",
    "\n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size, device=self.weight_mu.device)\n",
    "        return x.sign().mul_(x.abs().sqrt_())\n",
    "\n",
    "    def reset_noise(self):\n",
    "        epsilon_in = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "            return F.linear(x, weight, bias)\n",
    "        else:\n",
    "            return F.linear(x, self.weight_mu, self.bias_mu)\n",
    "\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        cur_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(cur_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            cur_dim = h_dim\n",
    "        self.back = nn.Sequential(*layers)\n",
    "\n",
    "        self.value_head = NoisyLinear(cur_dim, 1)\n",
    "        self.adv_head = NoisyLinear(cur_dim, output_dim)\n",
    "        self.n_actions = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        squeeze = False\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "            squeeze = True\n",
    "\n",
    "        z = self.back(x)\n",
    "        v = self.value_head(z)  # [B, 1]\n",
    "        a = self.adv_head(z)  # [B, A]\n",
    "        q = v + (a - a.mean(dim=1, keepdim=True))  # [B, A]\n",
    "\n",
    "        return q.squeeze(0) if squeeze else q\n",
    "\n",
    "    def reset_noise(self):\n",
    "        self.value_head.reset_noise()\n",
    "        self.adv_head.reset_noise()\n",
    "\n",
    "\n",
    "def create_dueling_network(input_dim, hidden_dims, output_dim):\n",
    "    return DuelingDQN(input_dim, hidden_dims, output_dim)\n",
    "\n",
    "\n",
    "class AcrobotDenseReward(gym.Wrapper):\n",
    "    def __init__(self, env, scale=0.5):\n",
    "        super().__init__(env)\n",
    "        # scale = 0.5 guarantees that max reward is -1 + 0.5 = -0.5\n",
    "        self.scale = scale\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        # obs = [cos(th1), sin(th1), cos(th2), sin(th2), th1_dot, th2_dot]\n",
    "        cos_th1, sin_th1, cos_th2, sin_th2 = obs[0], obs[1], obs[2], obs[3]\n",
    "\n",
    "        # height = -cos(th1) - cos(th1 + th2)\n",
    "        # cos(th1 + th2) = cos_th1*cos_th2 - sin_th1*sin_th2\n",
    "        height = -cos_th1 - (cos_th1 * cos_th2 - sin_th1 * sin_th2)\n",
    "\n",
    "        # Height in [-2, 2]\n",
    "        # Normalize into [0, 1]\n",
    "        height_norm = (height + 2.0) / 4.0\n",
    "\n",
    "        dense_bonus = self.scale * height_norm\n",
    "        shaped_reward = float(reward) + dense_bonus\n",
    "\n",
    "        return obs, shaped_reward, terminated, truncated, info"
   ],
   "id": "d0452eaf8c6ab8f8",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T20:21:42.148529Z",
     "start_time": "2026-02-19T20:21:42.139663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def push_nstep_transition(nstep_buf, replay_buffer, rb_pos, replay_buffer_size,\n",
    "                          gamma, max_prio):\n",
    "    \"\"\"\n",
    "    take first elem from nstep_buf and make n-step transition\n",
    "    return new rb_pos\n",
    "    \"\"\"\n",
    "    # (s, a, r, s_next, terminated, truncated)\n",
    "    s0, a0, _, _, _, _ = nstep_buf[0]\n",
    "\n",
    "    R = 0.0\n",
    "    sN = None\n",
    "    terminated_within = False\n",
    "    n_used = 0\n",
    "\n",
    "    for i, (s, a, r, s_next, terminated, truncated) in enumerate(nstep_buf):\n",
    "        R += (gamma ** i) * float(r)\n",
    "        sN = s_next\n",
    "        n_used = i + 1\n",
    "\n",
    "        if terminated:\n",
    "            terminated_within = True\n",
    "            break\n",
    "        if truncated:\n",
    "            break\n",
    "\n",
    "    item = (max_prio, s0, a0, R, sN, terminated_within, n_used)\n",
    "\n",
    "    if len(replay_buffer) < replay_buffer_size:\n",
    "        replay_buffer.append(item)\n",
    "    else:\n",
    "        replay_buffer[rb_pos] = item\n",
    "        rb_pos = (rb_pos + 1) % replay_buffer_size\n",
    "\n",
    "    nstep_buf.popleft()\n",
    "    return rb_pos\n"
   ],
   "id": "8c785c687d538e43",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "2bd1779b-b23a-4601-a878-c827173d2155",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88dcccfe-159d-48b3-adcb-d1d5459cdf84",
    "outputId": "8bdf1a24-30cd-4ad2-8dd4-88be070ac99d",
    "ExecuteTime": {
     "end_time": "2026-02-19T20:21:42.182817Z",
     "start_time": "2026-02-19T20:21:42.160251Z"
    }
   },
   "source": [
    "def compute_td_target_ddqn_nstep(Q, Q_slow, rewards, next_states, terminateds, n_steps, gamma=0.99):\n",
    "    r = to_tensor(rewards)  # [B]\n",
    "    s_next = to_tensor(next_states)  # [B, S]\n",
    "    term = to_tensor(terminateds, bool)  # [B]\n",
    "    n = to_tensor(n_steps, int).long()  # [B]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        a = Q(s_next).argmax(dim=1)  # [B]\n",
    "        q = Q_slow(s_next).gather(1, a.unsqueeze(1)).squeeze(1)  # [B]\n",
    "        gam = (gamma ** n.float())  # [B]\n",
    "        target = r + gam * q * (~term)\n",
    "    return target\n",
    "\n",
    "\n",
    "def run_ddqn_prioritized_rb(\n",
    "        env_name=\"CartPole-v1\",\n",
    "        hidden_dims=(256, 256), lr=1e-3, gamma=0.99,\n",
    "        total_max_steps=100_000,\n",
    "        train_schedule=4, replay_buffer_size=400, batch_size=32,\n",
    "        eval_schedule=1000, smooth_ret_window=1,\n",
    "        tau=0.005, success_ret=200.,\n",
    "        start_learn=10000, nstep=3, seed=None\n",
    "):\n",
    "    best_avg_return = -1e9\n",
    "    last_avg_return = None\n",
    "    best_state_dict = None\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    env = gym.make(env_name)\n",
    "    if seed is not None:\n",
    "        env.action_space.seed(seed)\n",
    "        s, _ = env.reset(seed=seed)\n",
    "    else:\n",
    "        s, _ = env.reset()\n",
    "    if env_name == \"Acrobot-v1\":  # action repeat\n",
    "        env = AcrobotDenseReward(env, scale=0.5)\n",
    "    replay_buffer = []\n",
    "    rb_pos = 0\n",
    "    nstep_buf = deque()\n",
    "\n",
    "    eval_return_history = deque(maxlen=smooth_ret_window)\n",
    "\n",
    "    full_eval_history = []\n",
    "\n",
    "    Q = create_dueling_network(\n",
    "        input_dim=env.observation_space.shape[0],\n",
    "        hidden_dims=hidden_dims,\n",
    "        output_dim=env.action_space.n\n",
    "    )\n",
    "    opt = torch.optim.Adam(Q.parameters(), lr=lr)\n",
    "\n",
    "    Q_slow = deepcopy(Q)\n",
    "\n",
    "    s = norm_state(s)\n",
    "    global_step = 1\n",
    "    max_prio = 1.0\n",
    "    Q.train()\n",
    "    for global_step in range(1, total_max_steps + 1):\n",
    "        beta = 0.4 + (1.0 - 0.4) * min(1.0, global_step / total_max_steps)\n",
    "        if global_step < start_learn:\n",
    "            a = env.action_space.sample()\n",
    "            epsilon = 1.0  # for logs\n",
    "        else:\n",
    "            Q.reset_noise()\n",
    "            a = select_action_eps_greedy(Q, s, epsilon=0.0)\n",
    "            epsilon = 0.0\n",
    "        s_next, r, terminated, truncated, _ = env.step(a)\n",
    "        s_next = norm_state(s_next)\n",
    "        done = terminated or truncated\n",
    "        nstep_buf.append((s, a, r, s_next, terminated, truncated))\n",
    "        # update after n steps\n",
    "        if len(nstep_buf) >= nstep:\n",
    "            rb_pos = push_nstep_transition(\n",
    "                nstep_buf, replay_buffer, rb_pos, replay_buffer_size,\n",
    "                gamma, max_prio\n",
    "            )\n",
    "\n",
    "        # update end states\n",
    "        if terminated or truncated:\n",
    "            while len(nstep_buf) > 0:\n",
    "                rb_pos = push_nstep_transition(\n",
    "                    nstep_buf, replay_buffer, rb_pos, replay_buffer_size,\n",
    "                    gamma, max_prio\n",
    "                )\n",
    "\n",
    "        if global_step >= start_learn and global_step % train_schedule == 0:\n",
    "            train_batch, indices, weights = sample_prioritized_batch(replay_buffer, batch_size, beta=beta)\n",
    "            (states, actions, rewards, next_states, terminateds, n_steps) = train_batch\n",
    "\n",
    "            opt.zero_grad()\n",
    "            Q.reset_noise()\n",
    "            Q_slow.reset_noise()\n",
    "            td_target = compute_td_target_ddqn_nstep(Q, Q_slow, rewards, next_states, terminateds, gamma=gamma,\n",
    "                                                     n_steps=n_steps)\n",
    "            loss, td_losses = compute_td_loss(Q, states, actions, td_target, weights=weights)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(Q.parameters(), max_norm=10.0)\n",
    "            opt.step()\n",
    "            for target_param, local_param in zip(Q_slow.parameters(), Q.parameters()):\n",
    "                target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "            update_batch(\n",
    "                replay_buffer, indices, td_losses.detach().abs().cpu().numpy().reshape(-1)\n",
    "            )\n",
    "            batch_max = float(td_losses.detach().max().item())\n",
    "            if batch_max > max_prio:\n",
    "                max_prio = batch_max\n",
    "\n",
    "        if global_step % eval_schedule == 0:\n",
    "            Q.eval()\n",
    "            eval_return = eval_dqn(env_name, Q)\n",
    "            Q.train()\n",
    "            eval_return_history.append(eval_return)\n",
    "            full_eval_history.append(eval_return)\n",
    "            avg_return = np.mean(eval_return_history)\n",
    "            last_avg_return = avg_return\n",
    "\n",
    "            if avg_return > best_avg_return:\n",
    "                best_avg_return = avg_return\n",
    "                best_state_dict = {k: v.detach().cpu().clone() for k, v in Q.state_dict().items()}\n",
    "            print(f'{global_step=} | {avg_return=:.3f} | {epsilon=:.3f}')\n",
    "            if avg_return >= success_ret:\n",
    "                print('Решено!')\n",
    "                break\n",
    "\n",
    "        s = s_next\n",
    "        if done:\n",
    "            s, _ = env.reset()\n",
    "            s = norm_state(s)\n",
    "    return {\n",
    "        \"best_avg_return\": float(best_avg_return),\n",
    "        \"last_avg_return\": None if last_avg_return is None else float(last_avg_return),\n",
    "        \"steps\": int(global_step),\n",
    "        \"best_state_dict\": best_state_dict,\n",
    "        \"eval_history\": list(full_eval_history)\n",
    "    }\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T20:55:22.556620Z",
     "start_time": "2026-02-19T20:21:42.200752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "BEST_CFG = dict(\n",
    "    env_name=\"Acrobot-v1\",\n",
    "    hidden_dims=(256, 256),\n",
    "    lr=3e-4,\n",
    "    gamma=0.99,\n",
    "    replay_buffer_size=100_000,\n",
    "    batch_size=256,\n",
    "    start_learn=10_000,\n",
    "    eval_schedule=5_000,\n",
    "    total_max_steps=1_000_000,\n",
    "    nstep=1,\n",
    "    train_schedule=2,\n",
    "    tau=0.001,\n",
    "    seed=4242\n",
    ")\n",
    "\n",
    "print(\"Starting training with optimal Rainbow-lite config...\")\n",
    "results = run_ddqn_prioritized_rb(**BEST_CFG)\n",
    "\n",
    "print(\"\\nTraining Finished!\")\n",
    "print(f\"Best Evaluation Return: {results['best_avg_return']:.2f}\")\n",
    "\n",
    "if \"best_state_dict\" in results:\n",
    "    _tmp_env = gym.make(BEST_CFG[\"env_name\"])\n",
    "    Q_final = create_dueling_network(\n",
    "        input_dim=_tmp_env.observation_space.shape[0],\n",
    "        hidden_dims=BEST_CFG[\"hidden_dims\"],\n",
    "        output_dim=_tmp_env.action_space.n\n",
    "    )\n",
    "    _tmp_env.close()\n",
    "\n",
    "    Q_final.load_state_dict(results[\"best_state_dict\"])\n",
    "    Q_final.eval()\n",
    "\n",
    "    final_score = eval_dqn(BEST_CFG[\"env_name\"], Q_final, n_episodes=100, seed=1000)\n",
    "    print(f\"Final SOTA Evaluation (100 episodes): {final_score:.2f}\")\n",
    "if \"eval_history\" in results:\n",
    "    history = results[\"eval_history\"]\n",
    "    eval_steps = np.arange(1, len(history) + 1) * BEST_CFG[\"eval_schedule\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(eval_steps, history, label=\"Rainbow-lite DQN\", color='blue', linewidth=2)\n",
    "    plt.axhline(y=-62, color='red', linestyle='--', label='Theoretical Physical Limit (~ -62)')\n",
    "\n",
    "    plt.title(\"Training Curve (Acrobot-v1)\", fontsize=14)\n",
    "    plt.xlabel(\"Environment Steps\", fontsize=12)\n",
    "    plt.ylabel(\"Evaluation Return (10 episodes avg)\", fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_curve.png\", dpi=300)\n",
    "    plt.show()"
   ],
   "id": "bb39ab6276c4df32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with optimal Rainbow-lite config...\n",
      "global_step=5000 | avg_return=-500.000 | epsilon=1.000\n",
      "global_step=10000 | avg_return=-500.000 | epsilon=0.000\n",
      "global_step=15000 | avg_return=-500.000 | epsilon=0.000\n",
      "global_step=20000 | avg_return=-500.000 | epsilon=0.000\n",
      "global_step=25000 | avg_return=-500.000 | epsilon=0.000\n",
      "global_step=30000 | avg_return=-240.800 | epsilon=0.000\n",
      "global_step=35000 | avg_return=-117.200 | epsilon=0.000\n",
      "global_step=40000 | avg_return=-475.200 | epsilon=0.000\n",
      "global_step=45000 | avg_return=-500.000 | epsilon=0.000\n",
      "global_step=50000 | avg_return=-500.000 | epsilon=0.000\n",
      "global_step=55000 | avg_return=-392.800 | epsilon=0.000\n",
      "global_step=60000 | avg_return=-500.000 | epsilon=0.000\n",
      "global_step=65000 | avg_return=-500.000 | epsilon=0.000\n",
      "global_step=70000 | avg_return=-99.400 | epsilon=0.000\n",
      "global_step=75000 | avg_return=-136.900 | epsilon=0.000\n",
      "global_step=80000 | avg_return=-94.800 | epsilon=0.000\n",
      "global_step=85000 | avg_return=-125.300 | epsilon=0.000\n",
      "global_step=90000 | avg_return=-96.800 | epsilon=0.000\n",
      "global_step=95000 | avg_return=-212.400 | epsilon=0.000\n",
      "global_step=100000 | avg_return=-419.700 | epsilon=0.000\n",
      "global_step=105000 | avg_return=-86.300 | epsilon=0.000\n",
      "global_step=110000 | avg_return=-80.700 | epsilon=0.000\n",
      "global_step=115000 | avg_return=-77.200 | epsilon=0.000\n",
      "global_step=120000 | avg_return=-76.500 | epsilon=0.000\n",
      "global_step=125000 | avg_return=-74.500 | epsilon=0.000\n",
      "global_step=130000 | avg_return=-78.600 | epsilon=0.000\n",
      "global_step=135000 | avg_return=-93.500 | epsilon=0.000\n",
      "global_step=140000 | avg_return=-81.400 | epsilon=0.000\n",
      "global_step=145000 | avg_return=-73.900 | epsilon=0.000\n",
      "global_step=150000 | avg_return=-76.600 | epsilon=0.000\n",
      "global_step=155000 | avg_return=-83.000 | epsilon=0.000\n",
      "global_step=160000 | avg_return=-74.900 | epsilon=0.000\n",
      "global_step=165000 | avg_return=-80.100 | epsilon=0.000\n",
      "global_step=170000 | avg_return=-75.900 | epsilon=0.000\n",
      "global_step=175000 | avg_return=-81.900 | epsilon=0.000\n",
      "global_step=180000 | avg_return=-71.100 | epsilon=0.000\n",
      "global_step=185000 | avg_return=-80.400 | epsilon=0.000\n",
      "global_step=190000 | avg_return=-72.400 | epsilon=0.000\n",
      "global_step=195000 | avg_return=-80.700 | epsilon=0.000\n",
      "global_step=200000 | avg_return=-76.400 | epsilon=0.000\n",
      "global_step=205000 | avg_return=-75.000 | epsilon=0.000\n",
      "global_step=210000 | avg_return=-77.300 | epsilon=0.000\n",
      "global_step=215000 | avg_return=-76.300 | epsilon=0.000\n",
      "global_step=220000 | avg_return=-70.100 | epsilon=0.000\n",
      "global_step=225000 | avg_return=-75.700 | epsilon=0.000\n",
      "global_step=230000 | avg_return=-78.100 | epsilon=0.000\n",
      "global_step=235000 | avg_return=-75.900 | epsilon=0.000\n",
      "global_step=240000 | avg_return=-73.300 | epsilon=0.000\n",
      "global_step=245000 | avg_return=-76.800 | epsilon=0.000\n",
      "global_step=250000 | avg_return=-71.400 | epsilon=0.000\n",
      "global_step=255000 | avg_return=-77.300 | epsilon=0.000\n",
      "global_step=260000 | avg_return=-69.000 | epsilon=0.000\n",
      "global_step=265000 | avg_return=-78.800 | epsilon=0.000\n",
      "global_step=270000 | avg_return=-71.900 | epsilon=0.000\n",
      "global_step=275000 | avg_return=-70.600 | epsilon=0.000\n",
      "global_step=280000 | avg_return=-74.200 | epsilon=0.000\n",
      "global_step=285000 | avg_return=-67.200 | epsilon=0.000\n",
      "global_step=290000 | avg_return=-68.200 | epsilon=0.000\n",
      "global_step=295000 | avg_return=-72.300 | epsilon=0.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m      1\u001B[39m BEST_CFG = \u001B[38;5;28mdict\u001B[39m(\n\u001B[32m      2\u001B[39m     env_name=\u001B[33m\"\u001B[39m\u001B[33mAcrobot-v1\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      3\u001B[39m     hidden_dims=(\u001B[32m256\u001B[39m, \u001B[32m256\u001B[39m),\n\u001B[32m   (...)\u001B[39m\u001B[32m     14\u001B[39m     seed=\u001B[32m4242\u001B[39m\n\u001B[32m     15\u001B[39m )\n\u001B[32m     17\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mStarting training with optimal Rainbow-lite config...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m results = \u001B[43mrun_ddqn_prioritized_rb\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mBEST_CFG\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mTraining Finished!\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     21\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mBest Evaluation Return: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresults[\u001B[33m'\u001B[39m\u001B[33mbest_avg_return\u001B[39m\u001B[33m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 96\u001B[39m, in \u001B[36mrun_ddqn_prioritized_rb\u001B[39m\u001B[34m(env_name, hidden_dims, lr, gamma, total_max_steps, train_schedule, replay_buffer_size, batch_size, eval_schedule, smooth_ret_window, tau, success_ret, start_learn, nstep, seed)\u001B[39m\n\u001B[32m     94\u001B[39m Q.reset_noise()\n\u001B[32m     95\u001B[39m Q_slow.reset_noise()\n\u001B[32m---> \u001B[39m\u001B[32m96\u001B[39m td_target = \u001B[43mcompute_td_target_ddqn_nstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mQ\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mQ_slow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrewards\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnext_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mterminateds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     97\u001B[39m \u001B[43m                                         \u001B[49m\u001B[43mn_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     98\u001B[39m loss, td_losses = compute_td_loss(Q, states, actions, td_target, weights=weights)\n\u001B[32m     99\u001B[39m loss.backward()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 8\u001B[39m, in \u001B[36mcompute_td_target_ddqn_nstep\u001B[39m\u001B[34m(Q, Q_slow, rewards, next_states, terminateds, n_steps, gamma)\u001B[39m\n\u001B[32m      5\u001B[39m n = to_tensor(n_steps, \u001B[38;5;28mint\u001B[39m).long()  \u001B[38;5;66;03m# [B]\u001B[39;00m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m     a = \u001B[43mQ\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms_next\u001B[49m\u001B[43m)\u001B[49m.argmax(dim=\u001B[32m1\u001B[39m)  \u001B[38;5;66;03m# [B]\u001B[39;00m\n\u001B[32m      9\u001B[39m     q = Q_slow(s_next).gather(\u001B[32m1\u001B[39m, a.unsqueeze(\u001B[32m1\u001B[39m)).squeeze(\u001B[32m1\u001B[39m)  \u001B[38;5;66;03m# [B]\u001B[39;00m\n\u001B[32m     10\u001B[39m     gam = (gamma ** n.float())  \u001B[38;5;66;03m# [B]\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1774\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1775\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1776\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1784\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1786\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1787\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1789\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1790\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 67\u001B[39m, in \u001B[36mDuelingDQN.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     64\u001B[39m     x = x.unsqueeze(\u001B[32m0\u001B[39m)\n\u001B[32m     65\u001B[39m     squeeze = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m67\u001B[39m z = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mback\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     68\u001B[39m v = \u001B[38;5;28mself\u001B[39m.value_head(z)  \u001B[38;5;66;03m# [B, 1]\u001B[39;00m\n\u001B[32m     69\u001B[39m a = \u001B[38;5;28mself\u001B[39m.adv_head(z)  \u001B[38;5;66;03m# [B, A]\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1774\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1775\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1776\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1784\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1786\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1787\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1789\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1790\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:253\u001B[39m, in \u001B[36mSequential.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    249\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    250\u001B[39m \u001B[33;03mRuns the forward pass.\u001B[39;00m\n\u001B[32m    251\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    252\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m253\u001B[39m     \u001B[38;5;28minput\u001B[39m = \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    254\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1774\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1775\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1776\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1784\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1786\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1787\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1789\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1790\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:143\u001B[39m, in \u001B[36mReLU.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    139\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m    140\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    141\u001B[39m \u001B[33;03m    Runs the forward pass.\u001B[39;00m\n\u001B[32m    142\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m143\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\PythonProject\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:1721\u001B[39m, in \u001B[36mrelu\u001B[39m\u001B[34m(input, inplace)\u001B[39m\n\u001B[32m   1719\u001B[39m     result = torch.relu_(\u001B[38;5;28minput\u001B[39m)\n\u001B[32m   1720\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1721\u001B[39m     result = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   1722\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "31f4ab6b-d997-4bf1-aca0-f69778510244",
    "242e0dc7-8596-4446-921e-7cb76fc72a0e",
    "1cfd75e8-6372-484b-9e8a-1121dcb9822f",
    "42cef2bf-7171-4bc4-9e03-4ca40b7d3a83",
    "90da8500-4981-4f20-bab7-299b337de4c6",
    "91e23ed3-ad53-4bf6-9586-615056aa8cd8",
    "3dc8331c-a511-4ae2-8935-6deae52c2030",
    "77b4c2f2-479f-4940-b2b0-c007d0b529dd",
    "df6fa045-56ea-4c5c-95c8-4f6efe11b9e9",
    "34023df6-54de-4c28-8913-6d7c51f66be1",
    "a2c4700b-032a-4a48-abbd-218ad2cbbcbb",
    "041bf63d-b50d-4131-b8ef-f45564162593",
    "cb4c2cdc-bf4c-4ca2-8296-df55e4a1725e",
    "db1c5d6b-42ac-4a7b-bed2-7f48c067c357"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
